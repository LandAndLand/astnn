{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('normal')",
   "display_name": "Python 3.8.6 64-bit ('normal')",
   "metadata": {
    "interpreter": {
     "hash": "f00aa4d08b5dbe9a4f4240f8270de016ce9f4f5bdff9dc19e01b094329a335d5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from pycparser import c_parser"
   ]
  },
  {
   "source": [
    "### 编译c源码为ast"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = c_parser.CParser()\n",
    "\n",
    "root = 'data/'\n",
    "language = 'c/'\n",
    "source = pd.read_pickle(root+language+'programs.pkl')\n",
    "#print(source)\n",
    "# label是指该段源代码来自于哪个类别吗？\n",
    "source.columns = ['id', 'code', 'label']\n",
    "print('parse starting')\n",
    "source['code'] = source['code'].apply(parser.parse)\n",
    "source.to_pickle(root+'c/ast.pkl')\n",
    "print('parse ending')\n",
    "#source_code = source['code'].iloc[0]\n",
    "#print(type(source_code))\n",
    "#ast= parser.parse(source_code)\n",
    "#ast.show()"
   ]
  },
  {
   "source": [
    "### 编译java源码为ast"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "def parse_program(func):\n",
    "    tokens = javalang.tokenizer.tokenize(func)\n",
    "    #print(f'tokens化得到的序列:{tokens}')\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    tree = parser.parse_member_declaration()\n",
    "    return tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "parse starting\n",
      "parse ending\n"
     ]
    }
   ],
   "source": [
    "from pycparser import c_parser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "root = 'data/'\n",
    "language = 'java/'\n",
    "source_java = pd.read_csv(root+language+'bcb_funcs_all.tsv',\n",
    "                            sep='\\t', \n",
    "                            header=None, \n",
    "                            encoding='utf-8')\n",
    "source_java.columns = ['id', 'code']\n",
    "print('parse starting')\n",
    "source_java['code'] = source_java['code'].apply(parse_program)\n",
    "source_java.to_pickle('data/java/ast.pkl')\n",
    "print('parse ending')\n",
    "#java = source_java['code'].iloc[2]\n",
    "#print(java)\n",
    "#source['code'] = source['code'].apply(parse_program)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### 分割pairs 为训练集、验证集、测试集"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def split_data(root = 'data/', ratio = '3:1:1', language = 'c'):\n",
    "  if language is 'c':\n",
    "    filename = 'oj_clone_ids.pkl'\n",
    "  else:\n",
    "    filename = 'bcb_pair_ids.pkl'\n",
    "  path = root + language + '/' + filename\n",
    "  data = pd.read_pickle(path)\n",
    "  data_num = len(data)\n",
    "  ratios = [int(r) for r in ratio.split(':')]\n",
    "  train_split = int(ratios[0]/sum(ratios)*data_num)\n",
    "  val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
    "  \n",
    "  #对要分割的数据进行随机打散\n",
    "  print('starting split')\n",
    "  data = data.sample(frac=1, random_state=666)\n",
    "\n",
    "  train = data.iloc[:train_split]\n",
    "  dev = data.iloc[train_split:val_split]\n",
    "  test = data.iloc[val_split:]\n",
    "\n",
    "  def check_or_create(path):\n",
    "    if not os.path.exists(path):\n",
    "      os.mkdir(path) \n",
    "  train_path = root+language+'/train/'\n",
    "  check_or_create(train_path)\n",
    "  assert Path(train_path).is_dir()\n",
    "  train_file_path = train_path + 'train.pkl'\n",
    "  train.to_pickle(train_file_path)\n",
    "\n",
    "  dev_path = root+language+'/dev/'\n",
    "  check_or_create(dev_path)\n",
    "  dev_file_path = dev_path + 'dev.pkl'\n",
    "  train.to_pickle(dev_file_path)\n",
    "\n",
    "  test_path = root+language+'/test/'\n",
    "  check_or_create(test_path)\n",
    "  test_file_path = test_path + 'test.pkl'\n",
    "  test.to_pickle(test_file_path)\n",
    "  print('end split')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "starting split\n",
      "end split\n",
      "starting split\n",
      "end split\n"
     ]
    }
   ],
   "source": [
    "split_data(language='c')\n",
    "split_data(language = 'java')"
   ]
  },
  {
   "source": [
    "### 路径设置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'c'\n"
   ]
  },
  {
   "source": [
    "### 读取c源码训练集所有的ast"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_ast(root = 'data/',language='c'):\n",
    "  source = pd.read_pickle(root+language+'/ast.pkl')\n",
    "  print(len(source['code']))\n",
    "  return source\n",
    "  # ast = source['code'].iloc[1]\n",
    "  # ast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52001\n"
     ]
    }
   ],
   "source": [
    "#source = read_ast('c')\n",
    "source_c_ast_DF = read_ast(language='c')\n",
    "#source_java_ast_DF = read_ast('java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[9566 8200 3897 ... 9254 3971 39727]\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "language = 'c'\n",
    "train_file_path = root + language + '/train/train.pkl'\n",
    "pairs = pd.read_pickle(train_file_path)\n",
    "# 找到训练集所包含的所有的source code id\n",
    "train_ids = pairs['id1'].append(pairs['id2']).unique()\n",
    "print(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from tree import ASTNode, SingleNode "
   ]
  },
  {
   "source": [
    "### 先序遍历得到ast所有的节点（为了进行word2vec训练）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func函数每次处理ast的当前遍历到的结点。即剩余子树的根节点\n",
    "def func(root,sequence):\n",
    "  #print(f'c: partial ast root: {root.__class__.__name__}')\n",
    "  current = SingleNode(root)\n",
    "  sequence.append(current.get_token())\n",
    "  #print(f'sequence: {sequence}')\n",
    "  for _, child in root.children():\n",
    "      func(child, sequence)\n",
    "  if current.get_token().lower() == 'compound':\n",
    "      sequence.append('End')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_sequences(ast):\n",
    "    sequence = []\n",
    "    func(ast, sequence)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trans_to_sequences' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9418433cf40d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0masts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast_DFs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_to_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trans_to_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# 设置ast_DFs的索引为id\n",
    "ast_copy = source_c_ast_DF.copy(False)\n",
    "\n",
    "# 得到训练集所包含的ast\n",
    "ast_DFs = ast_copy.set_index('id',drop=False).loc[train_ids]\n",
    "#print(ast_DFs)\n",
    "\n",
    "asts = ast_DFs['code']\n",
    "sequence = []\n",
    "corpus = asts.apply(trans_to_sequences)\n",
    "print(corpus)\n",
    "\n",
    "#print(type(corpus)) # Series\n",
    "str_corpus = [' '.join(c) for c in corpus]\n",
    "\n",
    "# 保存遍历得到的序列到文件\n",
    "#ast_seq = source_c_ast_DF.copy(False)\n",
    "#ast_seq['code'] = pd.Series(str_corpus)\n",
    "#ast_seq.to_pickle(root+language+'/train/ast_seq.pkl')"
   ]
  },
  {
   "source": [
    "### 使用训练集训练一个词典"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用word2vec从训练集中训练一个词典\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "embedding_size = 128\n",
    "w2v = Word2Vec(corpus, size=embedding_size, workers=16, sg=1, max_final_vocab=3000)\n",
    "w2v.save(root+language+'/train/embedding_astnode_w2v_' + str(embedding_size))"
   ]
  },
  {
   "source": [
    "# generate block sequences with index representations\n",
    "# 划分语句树（block），并使用训练好的词典初始化语句树的节点"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from prepare_data import get_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/'\n",
    "language = 'c'\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_index(node):\n",
    "        token = node.token\n",
    "        result = [vocab[token].index if token in vocab else max_token]\n",
    "        children = node.children\n",
    "        for child in children:\n",
    "            #print(f'tree_to_index: a node''s child: {child.token}')\n",
    "            result.append(tree_to_index(child))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2seq(ast):\n",
    "        blocks = []\n",
    "        get_blocks(ast, blocks)\n",
    "        tree = []\n",
    "        for b in blocks:\n",
    "            btree = tree_to_index(b)\n",
    "            tree.append(btree)\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate block sequences with index representations\n",
    "def generate_block_seqs(asts):\n",
    "    # 载入词典\n",
    "    word2vec = Word2Vec.load(root+language+'/train/embedding_astnode_w2v_' + str(embedding_size)).wv\n",
    "    vocab = word2vec.vocab\n",
    "    max_token = word2vec.syn0.shape[0]\n",
    "    \n",
    "    # source_c_ast_DF就是id code(ast形式) label \n",
    "    source_2_seq_DF =  asts.copy(False)\n",
    "    trees = source_2_seq_DF['code']\n",
    "    # 确认trees copy成功\n",
    "    #print(trees)\n",
    "    # trans2seq操作是在ast上进行的\n",
    "    # 把每一个ast转换成seq blocks\n",
    "    source_2_seq_DF['code'] = trees.apply(trans2seq)\n",
    "\n",
    "    if 'label' in source_2_seq_DF.columns:\n",
    "        source_2_seq_DF.drop('label', axis=1, inplace=True)\n",
    "\n",
    "    blocks = source_2_seq_DF\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datac/train/embedding_astnode_w2v_128'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-7e6d5b8c35a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_block_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_c_ast_DF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-855ac0b7887a>\u001b[0m in \u001b[0;36mgenerate_block_seqs\u001b[1;34m(asts)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_block_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 载入词典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/train/embedding_astnode_w2v_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmax_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1274\u001b[0m         \"\"\"\n\u001b[0;32m   1275\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m             \u001b[1;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m         \"\"\"\n\u001b[1;32m-> 1101\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ns_exponent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m     \"\"\"\n\u001b[1;32m-> 1355\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1356\u001b[0m         \u001b[1;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datac/train/embedding_astnode_w2v_128'"
     ]
    }
   ],
   "source": [
    "trees = generate_block_seqs(source_c_ast_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(data_path,part):\n",
    "    pairs = pd.read_pickle(data_path)\n",
    "    pairs['id1'] = pairs['id1'].astype(int)\n",
    "    pairs['id2'] = pairs['id2'].astype(int)\n",
    "    df = pd.merge(pairs, blocks, how='left', left_on='id1', right_on='id')\n",
    "    df = pd.merge(df, blocks, how='left', left_on='id2', right_on='id')\n",
    "    df.drop(['id_x', 'id_y'], axis=1,inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df.to_pickle(root+language+'/'+part+'/blocks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'c'\n",
    "train_file_path = 'clone/data/'+language+'/train/train.pkl'\n",
    "dev_file_path = 'clone/data/'+language+'/dev/train.pkl'\n",
    "test_file_path = 'clone/data/'+language+'/test/train.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge(train_file_path, 'train')\n",
    "merge(dev_file_path, 'dev')\n",
    "merge(test_file_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-d6d65541cb23>, line 2)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-d6d65541cb23>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    max_len = max(lens)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "lens = [len(item) for item in tree]\n",
    "        max_len = max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}