{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('normal')",
   "display_name": "Python 3.8.6 64-bit ('normal')",
   "metadata": {
    "interpreter": {
     "hash": "f00aa4d08b5dbe9a4f4240f8270de016ce9f4f5bdff9dc19e01b094329a335d5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 1\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 3\n",
    "USE_GPU = False\n",
    "root = 'data/'\n",
    "language = 'c'\n",
    "EMBEDDING_DIM = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(root+language+'/train/blocks.pkl').sample(frac=1)\n",
    "test_data = pd.read_pickle(root+language+'/test/blocks.pkl').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(root+language+\"/train/embedding_astnode_w2v_\"+str(EMBEDDING_DIM)).wv\n",
    "MAX_TOKENS = word2vec.syn0.shape[0]\n",
    "EMBEDDING_DIM = word2vec.syn0.shape[1]\n",
    "\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:word2vec.syn0.shape[0]] = word2vec.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "\n",
    "class BatchTreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
    "        super(BatchTreeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
    "        self.activation = F.relu\n",
    "        self.stop = -1\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.node_list = []\n",
    "        self.th = torch.cuda if use_gpu else torch\n",
    "        self.batch_node = None\n",
    "        self.max_index = vocab_size\n",
    "        # pretrained  embedding\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "            # self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def create_tensor(self, tensor):\n",
    "        if self.use_gpu:\n",
    "            return tensor.cuda()\n",
    "        return tensor\n",
    "\n",
    "    def traverse_mul(self, batch_btrees, batch_index):\n",
    "        len_btrees = len(batch_btrees)\n",
    "        if not len_btrees:\n",
    "            return None\n",
    "        batch_current = torch.zeros(len_btrees, self.embedding_dim)\n",
    "\n",
    "        index, children_index = [], []\n",
    "        current_node, children = [], []\n",
    "\n",
    "        # 遍历每一个语句树\n",
    "        for i in range(len_btrees):\n",
    "            # if node[i][0] is not -1:\n",
    "                index.append(i)\n",
    "                \n",
    "                # 每个语句树的第一个节点（父节点）\n",
    "                current_node.append(batch_btrees[i][0])\n",
    "                # 每个语句树除父节点以外所有节点（根节点的所有孩子节点） \n",
    "                temp = batch_btrees[i][1:]\n",
    "                \n",
    "                # 求出根节点的孩子结点的个数\n",
    "                c_num = len(temp)\n",
    "\n",
    "                # 遍历根节点的孩子节点\n",
    "                for j in range(c_num):\n",
    "                    if temp[j][0] is not -1:\n",
    "                        if len(children_index) <= j:\n",
    "                            children_index.append([i])\n",
    "                            children.append([temp[j]])\n",
    "                        else:\n",
    "                            children_index[j].append(i)\n",
    "                            children[j].append(temp[j])\n",
    "            # else:\n",
    "            #     batch_index[i] = -1\n",
    "\n",
    "        batch_current = self.W_c(batch_current.index_copy(0, self.th.LongTensor(index),\n",
    "                                                          self.embedding(self.th.LongTensor(current_node))))\n",
    "\n",
    "        for c in range(len(children)):\n",
    "            zeros = torch.zeros(len_btrees, self.encode_dim)\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
    "            tree = self.traverse_mul(children[c], batch_children_index)\n",
    "            if tree is not None:\n",
    "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
    "        # batch_index = [i for i in batch_index if i is not -1]\n",
    "        b_in = self.th.LongTensor(batch_index)\n",
    "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
    "        return batch_current\n",
    "\n",
    "    def forward(self, batch_btrees, bs):\n",
    "        self.batch_size = bs\n",
    "        #self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
    "        # 将每个语句树编码为一个固定维度\n",
    "        self.batch_node = torch.zeros(self.batch_size, self.encode_dim)\n",
    "        # print(self.batch_node)\n",
    "        self.node_list = []\n",
    "        self.traverse_mul(batch_btrees, list(range(self.batch_size)))\n",
    "        self.node_list = torch.stack(self.node_list)\n",
    "        return torch.max(self.node_list, 0)[0]\n",
    "\n",
    "\n",
    "class BatchProgramCC(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size, use_gpu=True, pretrained_weight=None):\n",
    "        super(BatchProgramCC, self).__init__()\n",
    "        self.stop = [vocab_size-1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "        self.gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_size = label_size\n",
    "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
    "                                        self.batch_size, self.gpu, pretrained_weight)\n",
    "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # linear\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
    "        # hidden\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.gpu is True:\n",
    "            if isinstance(self.bigru, nn.LSTM):\n",
    "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                return h0, c0\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def get_zeros(self, num):\n",
    "        zeros = torch.zeros(num, self.encode_dim)\n",
    "        if self.gpu:\n",
    "            return zeros.cuda()\n",
    "        return zeros\n",
    "\n",
    "    def encode(self, x):\n",
    "        # 一个batch中有多少个样本\n",
    "        \n",
    "        print(len(x))\n",
    "\n",
    "        # 遍历一个batch中的所有ast\n",
    "        for item in x:\n",
    "            # 得到每个ast中有多少个语句树\n",
    "            print(len(item))\n",
    "\n",
    "            # 打印每个语句树的内容\n",
    "            \"\"\" for i in item:\n",
    "                print(i) \"\"\"\n",
    "        \n",
    "        # 得到每个ast中有多少个语句树\n",
    "        lens = [len(item) for item in x]\n",
    "        # 取一个batch中的所有ast的 最大语句树个数\n",
    "        max_len = max(lens)\n",
    "        print(max_len)\n",
    "\n",
    "        encodes = []\n",
    "        # 对每一个样本i\n",
    "        for i in range(self.batch_size):\n",
    "            # 取样本i的每一个语句树j\n",
    "            for j in range(lens[i]):\n",
    "                # print(x[i][j])\n",
    "                encodes.append(x[i][j])\n",
    "        # encodes 得到了每个批次的所有语句树\n",
    "\n",
    "        encodes = self.encoder(encodes, sum(lens))\n",
    "        seq, start, end = [], 0, 0\n",
    "        for i in range(self.batch_size):\n",
    "            end += lens[i]\n",
    "            if max_len-lens[i]:\n",
    "                seq.append(self.get_zeros(max_len-lens[i]))\n",
    "                print(seq)\n",
    "            #print('*********')\n",
    "            seq.append(encodes[start:end])\n",
    "            #print(seq)\n",
    "            start = end\n",
    "        encodes = torch.cat(seq)\n",
    "        \"\"\" print('-----------------------')\n",
    "        print(seq) \"\"\"\n",
    "\n",
    "        # 最后输入到GRU层的数据格式：batch_size, 最大语句树个数, 每个语句树的嵌入维度（128）\n",
    "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
    "        # return encodes\n",
    "\n",
    "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        # gru_out = gru_out[:,-1]\n",
    "\n",
    "        return gru_out\n",
    "\n",
    "    # x1和x2都是输入的一个batch，每个batch包含：32个样本，每个样本是由ast拆分得到的所有语句树序列组成的\n",
    "    # 即：每个样本是一个完整的ast树拆分得到的语句树组成，这些语句树的每个结点都被word2vec嵌入表示\n",
    "    # ast的某个语句树的表示形式: [1, [21, [34, [50]], [138]]]\n",
    "    # 同一个list中的结点是兄弟结点, 临近的在不同list的结点是父母-孩子结点关系, 如21是1的孩子结点, 34是21的孩子结点\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        lvec, rvec = self.encode(x1), self.encode(x2)\n",
    "\n",
    "        # 一维范数计算两个编码的距离\n",
    "        abs_dist = torch.abs(torch.add(lvec, -rvec))\n",
    "\n",
    "        y = torch.sigmoid(self.hidden2label(abs_dist))\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import BatchProgramCC\n",
    "\n",
    "model = BatchProgramCC(EMBEDDING_DIM,\n",
    "                        HIDDEN_DIM,\n",
    "                        MAX_TOKENS+1,\n",
    "                        ENCODE_DIM,\n",
    "                        LABELS,\n",
    "                        BATCH_SIZE,\n",
    "                        USE_GPU, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adamax(parameters)\n",
    "loss_function = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    x1, x2, labels = [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        x2.append(item['code_y'])\n",
    "        labels.append([item['label']])\n",
    "    return x1, x2, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n71\n45\n22\n71\n*********\n[tensor([[0.0000, 0.1709, 0.0000,  ..., 0.2148, 0.0000, 0.0687],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.4827, 0.3540, 0.3587,  ..., 0.5540, 0.0000, 0.4836],\n        ...,\n        [0.0000, 0.1980, 0.1301,  ..., 0.2296, 0.0000, 0.1207],\n        [0.0000, 0.1381, 0.0080,  ..., 0.2438, 0.0000, 0.1302],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n[tensor([[0.0000, 0.1709, 0.0000,  ..., 0.2148, 0.0000, 0.0687],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.4827, 0.3540, 0.3587,  ..., 0.5540, 0.0000, 0.4836],\n        ...,\n        [0.0000, 0.1980, 0.1301,  ..., 0.2296, 0.0000, 0.1207],\n        [0.0000, 0.1381, 0.0080,  ..., 0.2438, 0.0000, 0.1302],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])]\n*********\n[tensor([[0.0000, 0.1709, 0.0000,  ..., 0.2148, 0.0000, 0.0687],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.4827, 0.3540, 0.3587,  ..., 0.5540, 0.0000, 0.4836],\n        ...,\n        [0.0000, 0.1980, 0.1301,  ..., 0.2296, 0.0000, 0.1207],\n        [0.0000, 0.1381, 0.0080,  ..., 0.2438, 0.0000, 0.1302],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0143, 0.4999, 0.0000,  ..., 0.6915, 0.0070, 0.2344],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n[tensor([[0.0000, 0.1709, 0.0000,  ..., 0.2148, 0.0000, 0.0687],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.4827, 0.3540, 0.3587,  ..., 0.5540, 0.0000, 0.4836],\n        ...,\n        [0.0000, 0.1980, 0.1301,  ..., 0.2296, 0.0000, 0.1207],\n        [0.0000, 0.1381, 0.0080,  ..., 0.2438, 0.0000, 0.1302],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0143, 0.4999, 0.0000,  ..., 0.6915, 0.0070, 0.2344],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])]\n*********\n[tensor([[0.0000, 0.1709, 0.0000,  ..., 0.2148, 0.0000, 0.0687],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.4827, 0.3540, 0.3587,  ..., 0.5540, 0.0000, 0.4836],\n        ...,\n        [0.0000, 0.1980, 0.1301,  ..., 0.2296, 0.0000, 0.1207],\n        [0.0000, 0.1381, 0.0080,  ..., 0.2438, 0.0000, 0.1302],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0143, 0.4999, 0.0000,  ..., 0.6915, 0.0070, 0.2344],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1395, 0.0000,  ..., 0.1257, 0.0000, 0.0000],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3806, 0.2965, 0.0000,  ..., 0.3210, 0.0000, 0.0598],\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n-----------------------\n[tensor([[0.0000, 0.1709, 0.0000,  ..., 0.2148, 0.0000, 0.0687],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.4827, 0.3540, 0.3587,  ..., 0.5540, 0.0000, 0.4836],\n        ...,\n        [0.0000, 0.1980, 0.1301,  ..., 0.2296, 0.0000, 0.1207],\n        [0.0000, 0.1381, 0.0080,  ..., 0.2438, 0.0000, 0.1302],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0143, 0.4999, 0.0000,  ..., 0.6915, 0.0070, 0.2344],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1395, 0.0000,  ..., 0.1257, 0.0000, 0.0000],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3806, 0.2965, 0.0000,  ..., 0.3210, 0.0000, 0.0598],\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n3\n33\n38\n44\n44\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])]\n*********\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]])]\n*********\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.9878, 1.6660, 0.0934,  ..., 1.4575, 0.0000, 0.4450],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n*********\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.9878, 1.6660, 0.0934,  ..., 1.4575, 0.0000, 0.4450],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.1837, 0.5314, 0.0000,  ..., 0.7398, 0.0070, 0.3678],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n-----------------------\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.3937, 0.1682, 0.0000,  ..., 0.4909, 0.0000, 0.0143],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.0000, 0.2447, 0.0000,  ..., 0.4586, 0.0000, 0.3258],\n        ...,\n        [0.9878, 1.6660, 0.0934,  ..., 1.4575, 0.0000, 0.4450],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>), tensor([[0.0000, 0.1357, 0.0000,  ..., 0.1969, 0.0000, 0.2399],\n        [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.0000, 0.0832],\n        [0.1837, 0.5314, 0.0000,  ..., 0.7398, 0.0070, 0.3678],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898],\n        [0.0387, 0.0278, 0.0000,  ..., 0.2070, 0.0000, 0.1638],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0898]],\n       grad_fn=<SliceBackward>)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "#from torch.utils.data import TensorDataset\n",
    "\n",
    "train_data_t, test_data_t = train_data.iloc[:3], test_data.iloc[:3]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    # training epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(train_data_t):\n",
    "        batch = get_batch(train_data_t, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        train1_inputs, train2_inputs, train_labels = batch\n",
    "        if USE_GPU:\n",
    "            train1_inputs, train2_inputs, train_labels = train1_inputs, train2_inputs, train_labels.cuda()\n",
    "\n",
    "        #model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train1_inputs, train2_inputs)\n",
    "\n",
    "        loss = loss_function(output, Variable(train_labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#print(\"Testing-%d...\"%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}